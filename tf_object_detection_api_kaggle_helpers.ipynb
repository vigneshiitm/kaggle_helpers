{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "tf-object-detection-api-kaggle-helpers.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vigneshiitm/kaggle_helpers/blob/main/tf_object_detection_api_kaggle_helpers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "wto6RlFX56Nh"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numba as nb\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path\n",
        "from time import time\n",
        "from numba import jit\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from os import listdir, makedirs, getcwd, remove\n",
        "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "from skimage.io import imread\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(111)\n",
        "color = sns.color_palette()\n",
        "%matplotlib inline\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"../input\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "trusted": true,
        "id": "AtFyMJh-56Nj"
      },
      "source": [
        "# Defining some paths as usual\n",
        "input_dir = Path('../input/')\n",
        "data_dir = input_dir / 'data_300x300/data_300x300'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5421cf76-536a-4ee3-902f-fe61eb5f803e",
        "_uuid": "676d75c81296fb305d4f2f0856708ba3bd311d8d",
        "id": "apA7dSgO56Nj"
      },
      "source": [
        "Quoting from the data description: `This dataset contains images and labels of feline reticulocytes (an immature red blood cell without a nucleus, having a granular or reticulated appearance when suitably stained). The dataset was created using equipment that is easily accessible to veterinarians: a standard laboratory microscope and two types of cameras: a basic microscope camera and a smartphone camera`\n",
        "\n",
        "Let's look how the dataset is arranged"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "51dad4c9-d7e0-4b81-b465-fad38f6e4f57",
        "_uuid": "937d3e27548047fab7c025fbf22e9618e370a3dd",
        "trusted": true,
        "id": "aqI2wYD356Nk"
      },
      "source": [
        "os.listdir(data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "af179a31-ddf8-49b5-8ad3-0d1d0a949da4",
        "_uuid": "35c7f5a7c3c59cb7f76737b732c7b2273549f0be",
        "collapsed": true,
        "id": "Sw3RQfwm56Nl"
      },
      "source": [
        "The `images` directory contains the images for training,  the `labels` directory contains the corresponding annotations for the training images and `TEST` contains the test images. How are the annotations done? When you annotate any object with a bounding box, there are certain things that you need to take care of in the annotations. The annotation corresponding to an image should contain the coordinates of the bounding boxes, the height of the image, the width of the image and the label corresponding to that box.  Here is an example of the annotation in our dataset:\n",
        "\n",
        "```\n",
        "<annotation>\n",
        "\t<folder>images</folder>\n",
        "\t<filename>000045.jpg</filename>\n",
        "\t<path>/home/vini/Desktop/data_300x300/images/000045.jpg</path>\n",
        "\t<source>\n",
        "\t\t<database>Unknown</database>\n",
        "\t</source>\n",
        "\t<size>\n",
        "\t\t<width>300</width>\n",
        "\t\t<height>300</height>\n",
        "\t\t<depth>3</depth>\n",
        "\t</size>\n",
        "\t<segmented>0</segmented>\n",
        "\t<object>\n",
        "\t\t<name>aggregate reticulocyte</name>\n",
        "\t\t<pose>Unspecified</pose>\n",
        "\t\t<truncated>0</truncated>\n",
        "\t\t<difficult>0</difficult>\n",
        "\t\t<bndbox>\n",
        "\t\t\t<xmin>140</xmin>\n",
        "\t\t\t<ymin>115</ymin>\n",
        "\t\t\t<xmax>169</xmax>\n",
        "\t\t\t<ymax>143</ymax>\n",
        "\t\t</bndbox>\n",
        "\t</object>\n",
        "\t<object>\n",
        "\t\t<name>punctate reticulocyte</name>\n",
        "\t\t<pose>Unspecified</pose>\n",
        "\t\t<truncated>0</truncated>\n",
        "\t\t<difficult>0</difficult>\n",
        "\t\t<bndbox>\n",
        "\t\t\t<xmin>72</xmin>\n",
        "\t\t\t<ymin>155</ymin>\n",
        "\t\t\t<xmax>103</xmax>\n",
        "\t\t\t<ymax>187</ymax>\n",
        "\t\t</bndbox>\n",
        "\t</object>\n",
        "\t<object>\n",
        "\t\t<name>erythrocyte</name>\n",
        "\t\t<pose>Unspecified</pose>\n",
        "\t\t<truncated>0</truncated>\n",
        "\t\t<difficult>0</difficult>\n",
        "\t\t<bndbox>\n",
        "\t\t\t<xmin>184</xmin>\n",
        "\t\t\t<ymin>195</ymin>\n",
        "\t\t\t<xmax>213</xmax>\n",
        "\t\t\t<ymax>228</ymax>\n",
        "\t\t</bndbox>\n",
        "\t</object>\n",
        "</annotation>\n",
        "```\n",
        "\n",
        "You can see the height and width of the image, the bounding box `bndbox`, the coordinates of the bounding box `xmin, ymin, xmax, ymax`, the label corresponding to that bounding box  given by the node `name` . There are a lot of opensource tools that you can use for annotating datasets but amongst all of them, the simplest and the best one is [labelImg](https://github.com/tzutalin/labelImg). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "12a33c4f-17e8-4dae-852e-f2133f1e768f",
        "_uuid": "edf16cfae8d346d7c65dab731644377578d7f11d",
        "id": "KYeLksPI56Nm"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "The annotations are given as `xmls`. The Tensorflow Object detection API accepts data in `TFRecords` format. So, we need to process our annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "2f007a46-f597-42a5-9f34-15c636d9a5a6",
        "_uuid": "2090c9d9f91352619776e695d74537fa177c415e",
        "collapsed": true,
        "trusted": true,
        "id": "va0tU4ZV56Nn"
      },
      "source": [
        "# A function to parse the xmls\n",
        "def parse_xmls(xml_files):\n",
        "    data = []\n",
        "    # Iterate over each file\n",
        "    for sample in xml_files:\n",
        "        # Get the xml tree\n",
        "        tree = ET.parse(sample)\n",
        "\n",
        "        # Get the root\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Get the members and extract the values\n",
        "        for member in root.findall('object'):\n",
        "            # Name of the image file\n",
        "            filename = root.find('filename').text\n",
        "            \n",
        "            # Height and width of the image\n",
        "            width =  int((root.find('size')).find('width').text)\n",
        "            height = int((root.find('size')).find('height').text)\n",
        "            \n",
        "            # Bounding box coordinates\n",
        "            bndbox = member.find('bndbox')\n",
        "            xmin = float(bndbox.find('xmin').text)\n",
        "            xmax = float(bndbox.find('xmax').text)\n",
        "            ymin = float(bndbox.find('ymin').text)\n",
        "            ymax = float(bndbox.find('ymax').text)\n",
        "            \n",
        "            # label to the corresponding bounding box\n",
        "            label =  member.find('name').text\n",
        "\n",
        "            data.append((filename, width, height, label, xmin, ymin, xmax, ymax))\n",
        "    \n",
        "    # Create a pandas dataframe\n",
        "    columns_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    df = pd.DataFrame(data=data, columns=columns_name)\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d107587c-f507-4457-a405-79e059d76587",
        "_uuid": "c3a2d9af6fef86241072c74c8df05d3638a08e96",
        "trusted": true,
        "id": "ER7SlEKr56Nn"
      },
      "source": [
        "images = sorted(glob.glob('../input/data_300x300/data_300x300/images/*.jpg'))\n",
        "xmls = sorted(glob.glob('../input/data_300x300/data_300x300/labels/*.xml'))\n",
        "print(\"Total number of images: \", len(images))\n",
        "print(\"Total number of xmls: \", len(xmls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "68c77d45-53f3-417a-b9a0-3fd5b3a97ffc",
        "_uuid": "196e55582ec8fba77bef68e4ae18b0c8ef31cfe0",
        "trusted": true,
        "id": "IKPmYQtE56No"
      },
      "source": [
        "# Parse the xmls and get the data in a dataframe\n",
        "df = parse_xmls(xmls)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "7f4bdd08-bb85-4c5d-b43b-2b7e6d4b5535",
        "_uuid": "d085d18f0f377a3256fd8862397a264903afff26",
        "trusted": true,
        "id": "NjRpgJA-56No"
      },
      "source": [
        "# How many classes do we have for object detection?\n",
        "label_counts = df['class'].value_counts()\n",
        "print(label_counts)\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "sns.barplot(x=label_counts.index, y= label_counts.values, color=color[2])\n",
        "plt.title('Labels in our dataset', fontsize=14)\n",
        "plt.xlabel('Label', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(range(len(label_counts.index)), ['erythrocyte', 'punctate reticulocyte', 'aggregate reticulocyte'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "db1561a5-9d7c-44c1-89f6-4d56f2ab0f0f",
        "_uuid": "9b5df8f1ce86e434b384edfa9d936b27153d1e37",
        "trusted": true,
        "id": "S2s8U7Cs56Nq"
      },
      "source": [
        "train, valid = train_test_split(df, test_size=0.2, stratify=df['class'], random_state=111)\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "valid = valid.reset_index(drop=True)\n",
        "print(\"Number of training samples: \", len(train))\n",
        "print(\"Number of validation samples: \", len(valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e02702ba-62ee-4964-b3b6-12d9b84b874d",
        "_uuid": "829d00d5287b37131bd6030ae4ecf89979c838c6",
        "id": "Xm74H54B56Nq"
      },
      "source": [
        "# The TensorFlow Object Detection API setup\n",
        "\n",
        "As I said there are certains things that needs to be installed on the host computer for using the API which is not possible in kernels, so I will demonstrate the steps in markdown. \n",
        "\n",
        "1.  Install tensorflow-gpu. Make sure you have installed the right version of Cuda and cuDNN. For more information, click [here](https://www.tensorflow.org/install/)\n",
        "2. Make a directory where you want to store all of the work and just cd into it\n",
        "3. Clone the tensorflow models repo `git clone https://github.com/tensorflow/models.git`\n",
        "4.  Install protobuf compiler `sudo apt-get install protobuf-compiler`\n",
        "5. Install other dependencies:\n",
        "    * pip install Cython\n",
        "    * pip install pillow\n",
        "    * pip install lxml\n",
        "    * pip install jupyter\n",
        "    * pip install matplotlib\n",
        " \n",
        "6.  `cd models/research/`\n",
        "\n",
        "7.   `protoc object_detection/protos/*.proto --python_out=.`\n",
        "\n",
        "8.   ```export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim```\n",
        "\n",
        "9.   Test your installation: `python object_detection/builders/model_builder_test.py`\n",
        "\n",
        "If the last step ran successfully then, you are done with the set up. Yay!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3c1123d2-455a-4471-ae16-6139ce31f673",
        "_uuid": "98ceb24fdba9905bcf1cc6482d1639128b221024",
        "id": "1rWvt-2I56Nq"
      },
      "source": [
        "# LabelMap\n",
        "\n",
        "Before converting you dataset to TFRecords, you need to make sure that you have a labelmap corresponding to all the labels that are in your dataset. For our dataset, the labelmap looks like this:\n",
        "\n",
        "```\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'erythrocyte'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'punctate reticulocyte'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 3\n",
        "  name: 'aggregate reticulocyte'\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "**Note**: Numbering starts from 1 as 0 is treated as background. I have named this labelmap as `bloodmap.pbtxt` and at this point my work directory looks like this:\n",
        "\n",
        "```\n",
        "/home\n",
        "     /ubuntu\n",
        "           /Nain\n",
        "                 /models\n",
        "                     /research\n",
        "                            /blood_train\n",
        "                                   /felina\n",
        "                                        /data_300x300\n",
        "                             bloodmap.pbtxt\n",
        "```\n",
        "\n",
        "`blood_train` is the directory that I created for this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cf62b777-4ba9-4c3f-b421-9f75fdb2e63d",
        "_uuid": "6f64b5661207c9a87ce8f8bccbcac2f6a322c33c",
        "id": "gePFcVT_56Nq"
      },
      "source": [
        "# Converting your data to TFRecords format\n",
        "\n",
        "This is the part where most of the the beginners get stuck. They have no clue how to do this. But this is quite simple. You have all your data information stored in the dataframe. The most importnat thing to remember is that a single image can contain multiple labels(bounding boxes) in the annotations, so you have to do a `groupby` on your dataframe. Let's see how we can do that.\n",
        "\n",
        "```python\n",
        "# Import the packages required\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import io\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mimg\n",
        "from PIL import Image\n",
        "from collections import namedtuple, OrderedDict\n",
        "from models.research.object_detection.utils import dataset_util\n",
        "from models.research.object_detection.utils import label_map_util\n",
        "\n",
        "# Function to group data and return the same\n",
        "# Group by imagefile name\n",
        "def make_groups(df, field=None):\n",
        "    if field==None:\n",
        "        field = 'filename'\n",
        "        \n",
        "    data = namedtuple('object', ['filename', 'info'])\n",
        "    grouped = df.groupby(field)\n",
        "    \n",
        "    grouped_data = []\n",
        "    for filename, x in zip(grouped.groups.keys(), grouped.groups):\n",
        "        grouped_data.append(data(filename, grouped.get_group(x)))\n",
        "        \n",
        "    return grouped_data\n",
        "    \n",
        "    \n",
        "  # Creating a tf record sample\n",
        "  def create_tf_example(group, img_path, label_map_dict)\n",
        "      # Read the imagefile. This will be used in features later \n",
        "      with tf.gfile.GFile(os.path.join(img_path, '{}'.format(group.filename)), 'rb') as f:\n",
        "          img_file = f.read()\n",
        "    \n",
        "      # Encode to bytes and read using PIL. Could be done directly too\n",
        "      encoded_img = io.BytesIO(img_file)\n",
        "      # Read the image using PIL\n",
        "      img = Image.open(encoded_img)\n",
        "      width, height = img.size\n",
        "    \n",
        "      # Encode the name of the img file\n",
        "      filename = group.filename.encode('utf8')\n",
        "      \n",
        "      # Define the format of the image file\n",
        "      img_format = b'jpg'   # The name will be in bytes\n",
        "    \n",
        "    \n",
        "      # Define the variables that you need as features\n",
        "      xmins = []\n",
        "      xmaxs = []\n",
        "      ymins = []\n",
        "      ymaxs = []\n",
        "      classes_text = []\n",
        "      classes = []\n",
        "\n",
        "      # Iterate over the namedtuple object\n",
        "      for index, row in group.info.iterrows():\n",
        "          xmins.append(row['xmin'] / width)   # store normalized values for bbox\n",
        "          xmaxs.append(row['xmax'] / width)\n",
        "          ymins.append(row['ymin'] / height)\n",
        "          ymaxs.append(row['ymax'] / height)\n",
        "          classes_text.append(row['class'].encode('utf8'))\n",
        "          classes.append(label_map_dict[row['class']])\n",
        "\n",
        "      tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "          'image/height': dataset_util.int64_feature(height),\n",
        "          'image/width': dataset_util.int64_feature(width),\n",
        "          'image/filename': dataset_util.bytes_feature(filename),\n",
        "          'image/source_id': dataset_util.bytes_feature(filename),\n",
        "          'image/encoded': dataset_util.bytes_feature(img_file),\n",
        "          'image/format': dataset_util.bytes_feature(img_format),\n",
        "          'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "          'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "          'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "          'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "          'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "          'image/object/class/label': dataset_util.int64_list_feature(classes),}))\n",
        "    \n",
        "      return tf_example\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "294a3c5c-d290-4e41-ac5f-5ef1b19b1cb9",
        "_uuid": "1731015af1999959db56b47fb7f1006c9cff23db",
        "id": "XODuLd0c56Nr"
      },
      "source": [
        "Great!! We have defined all the functions required for preprocessing and all. While creating TFRecords, all we need to do is to open a TFRecords writer instance and create `train.record` and `valid.record` from our `train` and `valid` dataframes. Let's do that.\n",
        "\n",
        "```python\n",
        "# Path where all the images are present\n",
        "img_path = './felina/data_300x300/images/'\n",
        "# Label map\n",
        "label_map_dict = label_map_util.get_label_map_dict('./bloodmap.pbtxt')\n",
        "\n",
        "writer = tf.python_io.TFRecordWriter('./train.record')\n",
        "# create groups in the df. One image may contain several instances of an object hence the grouping thing\n",
        "img_groups = make_groups(train, field='filename')\n",
        "# Iterate over the samples in each group create a TFRecord\n",
        "for group in img_groups:\n",
        "    tf_example = create_tf_example(group, img_path, label_map_dict)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "# close the writer\n",
        "writer.close()\n",
        "print(\"TFRecords for training data  created successfully\")\n",
        "\n",
        "\n",
        "writer = tf.python_io.TFRecordWriter('./valid.record')\n",
        "# create groups \n",
        "img_groups = make_groups(valid, field='filename')\n",
        "# Iterate over the samples in each group create a TFRecord\n",
        "for group in img_groups:\n",
        "    tf_example = create_tf_example(group, img_path, label_map_dict)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "# close the writer\n",
        "writer.close()\n",
        "print(\"TFRecords for validation data created successfully\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "89548f1a-422c-457d-9dd5-d7fc4e6cd98b",
        "_uuid": "bb3d2e5d4e3108c41deb93a075a183f961e926c5",
        "id": "9U6fecJk56Nr"
      },
      "source": [
        "# Adding the model you want to use \n",
        "\n",
        "Now we have almost everything ready.  We need to do two more steps:\n",
        "* Choosing the model config file that you want to use for training\n",
        "* Downloading the weights of the same model from [TensorFlow detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)\n",
        "\n",
        "Let's do this.\n",
        "1.  I chose the ssd_inception_v2 model for my training but you can chose whichever you like. `cp models/research/object_detection/samples/configs/ssd_inception_v2_coco.config`\n",
        "\n",
        "2. `wget download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz`\n",
        "\n",
        "3.  `unzip ssd_inception_v2_coco_2017_11_17.tar.gz`\n",
        "\n",
        "4. `mv ssd_inception_v2_coco_2017_11_17 ssd_inceptionv2`\n",
        "\n",
        "Now your work directory should be like this:\n",
        "\n",
        "```\n",
        "/home\n",
        "     /ubuntu\n",
        "           /Nain\n",
        "                 /models\n",
        "                     /research\n",
        "                            /blood_train\n",
        "                                /felina\n",
        "                                    /data_300x300\n",
        "                                 /ssd_inceptionv2      \n",
        "                                 bloodmap.pbtxt\n",
        "                                 train.record\n",
        "                                 valid.record\n",
        "                                 ssd_inception_v2_coco.config\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2a374161-ef15-46cb-8ae1-64dbd4bcc346",
        "_uuid": "11037f5b3b37462524982919722e019f43adef80",
        "id": "RlopeMVz56Nr"
      },
      "source": [
        "# Configuring your model config file\n",
        "\n",
        "In the model config that you chose to use, you need to make some changes. You need to give the path of the `tfrecords` and the `labelmap` files as well as the checkpoint of that model for fine tuning. Open your config file and edit the following lines:\n",
        "\n",
        "\n",
        "```\n",
        "num_classes: 3\n",
        "\n",
        "\n",
        "fine_tune_checkpoint:\"/home/ubuntu/Nain/models/research/blood_train/ssd_inceptionv2/model.ckpt\"\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/home/ubuntu/Nain/models/research/blood_train/train.record\"\n",
        "  }\n",
        "  label_map_path: \"/home/ubuntu/Nain/models/research/blood_train/bloodmap.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/home/ubuntu/Nain/models/research/blood_train/valid.record\"\n",
        "  }\n",
        "  label_map_path:\"/home/ubuntu/Nain/models/research/blood_train/bloodmap.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5a98ea33-9eb6-4992-9545-cb45583cb1ef",
        "_uuid": "19c33cc0fad2d4d06d4baa3662180475cf8eff73",
        "id": "p00PexOy56Nr"
      },
      "source": [
        "# Training\n",
        "\n",
        "We are almost done!! I know it's too much in one go but once you do it, you will become very comfortable in using it. To start the training, we need to do two things:\n",
        "* Create a directory for storing training checkpoints. I named it `checkpoints`\n",
        "* Copy the `train.py`,  `eval.py` and  `export_inference_graph.py` from the `object_detection` directory to our current directory\n",
        "\n",
        "This is how your things should be arranged by now:\n",
        "\n",
        "```\n",
        "/home\n",
        "     /ubuntu\n",
        "           /Nain\n",
        "                 /models\n",
        "                     /research\n",
        "                            /blood_train\n",
        "                                /felina\n",
        "                                    /data_300x300\n",
        "                                 /ssd_inceptionv2\n",
        "                                 /checkpoints\n",
        "                                 bloodmap.pbtxt\n",
        "                                 train.record\n",
        "                                 valid.record\n",
        "                                 ssd_inception_v2_coco.config\n",
        "                                 train.py\n",
        "                                 eval.py\n",
        "                                 export_inference_graph.py\n",
        "```\n",
        "\n",
        "And the final command to run the training!!!!\n",
        "\n",
        "```\n",
        "python train.py --logtostderr --train_dir=/home/ubuntu/Nain/models/research/blood_train/checkpoints/ --pipeline_config_path=/home/ubuntu/Nain/models/research/blood_train/ssd_inception_v2_coco.config\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f1862aa7-0182-4498-8636-e4e7b87d1b0e",
        "_uuid": "cfff0da8174c0bcbcbfd8a83f7c1f04074a5e037",
        "id": "IS1xRBDc56Ns"
      },
      "source": [
        "# Freezing the graph\n",
        "\n",
        "Once you are done with the training, you need to freeze the graph for doing inference.  The checkpoint depends on the number of iterations you completed for training.  I completed 25K iterations for this, but you should do more as it's not enough.  Freeze the graph:\n",
        "\n",
        "```\n",
        "python export_inference_graph.py --input_type image_tensor --pipeline_config_path /home/ubuntu/Nain/models/research/blood_train/ssd_inception_v2_coco.config --trained_checkpoint_prefix ./checkpoints/model.ckpt-25823 --output_directory ./fine_tuned_model\n",
        "```"
      ]
    }
  ]
}